{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["N0qZeBwc2vwi"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# MNIST OCR Model\n","##### Taken from [here](https://towardsdatascience.com/handwritten-digit-mnist-pytorch-977b5338e627)\n","##### This multi-digit mnist ocr uses our original MNIST classifier model on multi digit images that we split into single digit images and classify seperately. In order to use this notebook, train the model in [this](https://colab.research.google.com/drive/15_s6DFZJZFFgFvhzl0Xkm7-uGurq288N?usp=sharing#) notebook, save it, and load it in here."],"metadata":{"id":"CTFTFoU30i6I"}},{"cell_type":"markdown","source":["## Initial setup"],"metadata":{"id":"N0qZeBwc2vwi"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"M7ZL43nbtW-P","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671002470033,"user_tz":-120,"elapsed":7263,"user":{"displayName":"Raz Oren","userId":"12395684464361782065"}},"outputId":"80e7c2b1-8b86-49cf-b7dd-5014fc91d061"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["# imports and utils\n","%matplotlib inline\n","%config InlineBackend.figure_format = 'retina'\n","\n","import os\n","from time import time\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","from torch import nn, optim\n","\n","import torchvision\n","from torchvision import datasets, transforms\n","from google.colab import drive\n","\n","drive.mount('/content/gdrive')\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","def view_classify(img, ps):\n","    ''' Function for viewing an image and it's predicted classes.'''\n","    ps = ps.data.numpy().squeeze()\n","\n","    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n","    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n","    ax1.axis('off')\n","    ax2.barh(np.arange(10), ps)\n","    ax2.set_aspect(0.1)\n","    ax2.set_yticks(np.arange(10))\n","    ax2.set_yticklabels(np.arange(10))\n","    ax2.set_title('Class Probability')\n","    ax2.set_xlim(0, 1.1)\n","    plt.tight_layout()"]},{"cell_type":"code","source":["# Define a transform to normalize the data\n","transform = transforms.Compose([transforms.ToTensor(),\n","                              transforms.Normalize((0.5,), (0.5,)),\n","                              ])\n","\n","# Download and load the training data\n","trainset = datasets.MNIST('gdrive/MyDrive/mnist/MNIST_data/', download=True, train=True, transform=transform)\n","valset = datasets.MNIST('gdrive/MyDrive/mnist/MNIST_data/', download=True, train=False, transform=transform)\n","\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n","valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=True)"],"metadata":{"id":"u6FCfgpatr5S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1_-7rselffO0","executionInfo":{"status":"ok","timestamp":1671002361261,"user_tz":-120,"elapsed":2412,"user":{"displayName":"Raz Oren","userId":"12395684464361782065"}},"outputId":"52c66e89-53f3-4333-cdb0-dad6e4eead9d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["## Create (or Load) Multi-Digit Dataset"],"metadata":{"id":"ssrlWo5BoVlW"}},{"cell_type":"code","source":["class NumberDataset(torchvision.datasets.MNIST) :\n","\n","  def __init__(self, num_to_generate=120000, num_of_digits=4, im_width=28, im_height=28, train=True, download=True) :\n","    \"\"\"\n","    Args : \n","      num_of_digits (int) : the number of digits in each number\n","      im_width (int) : the width of a single digit image\n","      im_height (int) : the height of a single digit image\n","      train (bool) : if True create the images from the training set\n","      download (bool) : if True downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.\n","    \"\"\"\n","\n","    self.transform = transforms.Compose([\n","                                torchvision.transforms.ToTensor(),\n","                                torchvision.transforms.Normalize((0.5,),(0.5,)),\n","    ])\n","    self.data = torch.utils.data.DataLoader(\n","        datasets.MNIST('gdrive/MyDrive/mnist/MNIST_data/', train=train, download=download, \n","                                  transform=self.transform),\n","                                  batch_size=4, shuffle=True)\n","    \n","    self.res = []\n","\n","    for i in range(num_to_generate):\n","      if ((i % 1000) == 0) : \n","        print(\"Done {} numbers\".format(i))\n","      digits, vals = next(iter(self.data))\n","      target = 0\n","      image = torch.transpose(torch.reshape(torch.transpose(digits, 2,3), (1, num_of_digits * im_width, im_height)), 1,2)\n","      for j in range(num_of_digits): \n","        target = target + vals[j] * pow(10, num_of_digits-1-j)\n","      self.res.append((image, target))"],"metadata":{"id":"EASL5lZColqT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_of_digits = 4\n","im_width = 28\n","im_height = 28\n","batch_size_train = 1000\n","batch_size_test = 600\n","log_interval = 10\n","\n","random_seed = 1\n","torch.backends.cudnn.enabled = False\n","torch.manual_seed(random_seed)\n","\n","train_data = NumberDataset(num_to_generate=batch_size_train, num_of_digits=num_of_digits, im_width=im_width, im_height=im_height).res\n","train_size = len(train_data)\n","\n","print(\"Done proccessing training set, got {} numbers\".format(train_size))\n","\n","test_data = NumberDataset(num_to_generate=batch_size_test, num_of_digits=num_of_digits, im_width=im_width, im_height=im_height, train=False).res\n","test_size = len(test_data)\n","print(\"Done proccessing test set, got {} numbers\".format(test_size))"],"metadata":{"id":"rA8z8REUopSS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualize the Data Set\n","fig2, axes = plt.subplots(3,3)\n","fig2.tight_layout()\n","for i in range(9):\n","  sub = axes[int(i/3), i%3]\n","  sub.imshow(train_data[i][0][0], cmap='gray', interpolation='none')\n","  sub.set_title(\"Ground Truth: {}\".format(train_data[i][1])) \n","  sub.set_xticks([])\n","  sub.set_yticks([])"],"metadata":{"id":"9fIEZSqDxSkN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.save(train_data, 'gdrive/MyDrive/mnist/4_digit_model/mnist_4_digit_train_data')\n","torch.save(test_data, 'gdrive/MyDrive/mnist/mnist_4_digit_test_data')"],"metadata":{"id":"jWI7mJbXDile"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data = torch.load('gdrive/MyDrive/mnist/mnist_4_digit_train_data')\n","test_data = torch.load('gdrive/MyDrive/mnist/mnist_4_digit_test_data')"],"metadata":{"id":"ZgvFVBMet8yK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainloader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n","valloader = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=True)"],"metadata":{"id":"B0vKJRY1PeKM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Load OCR model - start here if you want to load an existing model\n"],"metadata":{"id":"NDKiEedb1ClW"}},{"cell_type":"code","source":["# Load model\n","model = torch.load('gdrive/My Drive/mnist/mnist_ocr_model')"],"metadata":{"id":"3EG-0F3t1GD1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Evaluate the model"],"metadata":{"id":"n1F5iVF01zv5"}},{"cell_type":"code","source":["def OCR(num_digits: int, valloader):\n","  images, labels = next(iter(valloader))\n","  \n","  figure = plt.figure()\n","  plt.axis('off')\n","  plt.imshow(images[0].numpy().squeeze(), cmap='gray_r')\n","\n","  # Split the n-digit image into n same equal parts\n","  for i in range(num_digits):\n","    img = images[0].view(28, 28 * num_digits)\n","    single_digit = img[:, (i)*28:(i+1)*28]\n","    single_digit_reshaped = single_digit.reshape(1, 28*28)\n","\n","    # Turn off gradients to speed up this part\n","    with torch.no_grad():\n","        logps = model(single_digit_reshaped)\n","\n","    # Output of the network are log-probabilities, need to take exponential for probabilities\n","    ps = torch.exp(logps)\n","    probab = list(ps.numpy()[0])\n","    print(\"Predicted Digit =\", probab.index(max(probab)))\n","    view_classify(single_digit_reshaped.view(1, 28, 28), ps)"],"metadata":{"id":"6qgv7aXTEPnh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["OCR(num_digits=4, valloader=valloader)"],"metadata":{"id":"uDK-xII-FZfV","colab":{"base_uri":"https://localhost:8080/","height":519},"executionInfo":{"status":"error","timestamp":1671003103108,"user_tz":-120,"elapsed":697,"user":{"displayName":"Raz Oren","userId":"12395684464361782065"}},"outputId":"c74e9197-bdc2-4233-a45e-70b9690dafbe"},"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-2f418a3ba050>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mOCR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_digits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-21-e93cd4f96aa0>\u001b[0m in \u001b[0;36mOCR\u001b[0;34m(num_digits, valloader)\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;31m# Split the n-digit image into n same equal parts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_digits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_digits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0msingle_digit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0msingle_digit_reshaped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msingle_digit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: shape '[28, 84]' is invalid for input of size 784"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"image/png":{"width":231,"height":231},"needs_background":"light"}}]}]}